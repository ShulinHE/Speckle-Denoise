{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from data import read_data\n",
    "from utils import add_noise_est, normalize, add_noise, squeeze_patches\n",
    "\n",
    "gpu_ok = tf.test.is_gpu_available()\n",
    "print(\"tf version:\", tf.__version__)\n",
    "print(\"use GPU:\", gpu_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# 避免显卡显存小而报错，显存自适应分配\n",
    "physical_devices=tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "\n",
    "# 给显存分配几个子虚拟内存\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    physical_devices[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "\n",
    "# 发现容易报错This is probably because cuDNN failed to initialize，是显存还是不够的原因"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "## 1. DIV2K data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_X_p, train_Y_p), (test_X_p, test_Y_p) = read_data('div2k')\n",
    "N_ims= len(train_X_p)\n",
    "\n",
    "train_X_p, label_train_X_p = squeeze_patches(train_X_p)\n",
    "train_Y_p, label_train_Y_p = squeeze_patches(train_Y_p)\n",
    "test_X_p, label_test_X_p = squeeze_patches(test_X_p)\n",
    "test_Y_p, label_test_Y_p = squeeze_patches(test_Y_p)\n",
    "\n",
    "train_X_p = train_X_p[:,np.newaxis,...]\n",
    "train_Y_p = train_Y_p[...,np.newaxis]\n",
    "test_X_p = test_X_p[:,np.newaxis,...]\n",
    "test_Y_p = test_Y_p[...,np.newaxis]\n",
    "\n",
    "print('\\nTrain data:')\n",
    "print('train_X_p:',train_X_p.shape)\n",
    "print('train_Y_p:',train_Y_p.shape)\n",
    "\n",
    "print('\\nTest data:')\n",
    "print('test_X_p:',test_X_p.shape)\n",
    "print('test_Y_p:',test_Y_p.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ultrasound data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "(train_X_p, train_Y_p), (test_X_p, test_Y_p) = read_data('ultrasound')\n",
    "N_ims= len(train_X_p)\n",
    "\n",
    "train_X_p, label_train_X_p = squeeze_patches(train_X_p)\n",
    "train_Y_p, label_train_Y_p = squeeze_patches(train_Y_p)\n",
    "test_X_p, label_test_X_p = squeeze_patches(test_X_p)\n",
    "test_Y_p, label_test_Y_p = squeeze_patches(test_Y_p)\n",
    "\n",
    "train_X_p = train_X_p[:,np.newaxis,...,np.newaxis]\n",
    "train_Y_p = train_Y_p[...,np.newaxis]\n",
    "test_X_p = test_X_p[:,np.newaxis,...,np.newaxis]\n",
    "test_Y_p = test_Y_p[...,np.newaxis]\n",
    "\n",
    "print('\\nTrain data:')\n",
    "print('train_X_p:',train_X_p.shape)\n",
    "print('train_Y_p:',train_Y_p.shape)\n",
    "\n",
    "print('\\nTest data:')\n",
    "print('test_X_p:',test_X_p.shape)\n",
    "print('test_Y_p:',test_Y_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Visualization'''\n",
    "N_show = 20\n",
    "\n",
    "plt.figure(figsize = (10,5*N_show))\n",
    "for i in range(N_show):\n",
    "    n = np.random.randint(test_X_p.shape[0], size = 1)\n",
    "\n",
    "    plt.subplot(N_show, 2, 2*i+1)\n",
    "    plt.imshow(train_X_p[n][...,0].squeeze(), cmap='gray')\n",
    "    #plt.title('noise map std: {:.3f}'.format(train_X_p[n][...,1].std()))\n",
    "    \n",
    "    plt.subplot(N_show, 2, 2*i+2)\n",
    "    plt.imshow(train_Y_p[n].squeeze(), cmap='gray')\n",
    "    \n",
    "#plt.savefig('exemple.png')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_X_p.mean(), test_X_p.std(), test_X_p.min(), test_X_p.max())\n",
    "print(test_Y_p.mean(), test_Y_p.std(), test_Y_p.min(), test_Y_p.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_noise_map = False   # if True, concatenate a noise map to the input\n",
    "#use_noise_est = False   # if True, use a model to estimate noise map, if False, use known info\n",
    "\n",
    "if not use_noise_map:\n",
    "    train_X_p = train_X_p[...,0][..., np.newaxis]\n",
    "    test_X_p = test_X_p[...,0][..., np.newaxis]\n",
    "\n",
    "#train_X_p = train_X_p.squeeze(1)\n",
    "#test_X_p = test_X_p.squeeze(1)\n",
    "    \n",
    "print('Train data:')\n",
    "print('train_X_p:',train_X_p.shape)\n",
    "print('train_Y_p:',train_Y_p.shape)\n",
    "\n",
    "print('\\nTest data:')\n",
    "print('test_X_p:',test_X_p.shape)\n",
    "print('test_Y_p:',test_Y_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = train_X_p.shape[0]\n",
    "N_test = test_X_p.shape[0]\n",
    "\n",
    "# training hyperparameters\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.data API to shuffle and batch data.\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_X_p,test_Y_p))\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method \n",
    "## Md1: kernel simulation by the features of patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('kernels/patches50.txt', 'rb') as f:\n",
    "    patches_copy = pickle.loads(f.read())\n",
    "    patches = []\n",
    "    for k,v in patches_copy.items():\n",
    "        patches.append(v)\n",
    "    patches = np.stack(patches, axis=0)\n",
    "        \n",
    "with open('kernels/kernels50.txt', 'rb') as f:\n",
    "    kernels_copy = pickle.loads(f.read())\n",
    "    kernels = []\n",
    "    for k,v in kernels_copy.items():\n",
    "        kernels.append(v)\n",
    "    kernels = np.stack(kernels, axis=0)\n",
    "    \n",
    "print(patches.shape)\n",
    "print(kernels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the data with the hybrid method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Plusieurs critères sont possibles: NCC, SSIM, ou selon la direction***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_patches(batch_test_X_flatten, learned_patches):\n",
    "    '''\n",
    "    simulate the patches by the few learned patches and return the labels of their corresponding kernels\n",
    "    batch_test_X_flatten: (nb_patches, 9)\n",
    "    learned_patches: (nb_learned_patches, 9)\n",
    "    '''\n",
    "    batch_test_X_flatten_normalized = (batch_test_X_flatten-tf.reduce_mean(batch_test_X_flatten, axis=-1, keepdims=True)) / (tf.math.reduce_std(batch_test_X_flatten, axis=-1, keepdims=True))\n",
    "    learned_patches_normalized = (learned_patches-tf.reduce_mean(learned_patches, axis=-1, keepdims=True)) / (tf.math.reduce_std(learned_patches, axis=-1, keepdims=True))\n",
    "    \n",
    "    score = tf.matmul(batch_test_X_flatten_normalized, tf.transpose(learned_patches_normalized))\n",
    "    labels = tf.math.argmax(score, axis=-1)\n",
    "\n",
    "    return labels\n",
    "\n",
    "def apply_filtering(frames, core, kernel_size):\n",
    "    img_stack = []\n",
    "    pred_img = []\n",
    "    kernel = kernel_size[::-1]\n",
    "    for index, K in enumerate(kernel):\n",
    "        if not len(img_stack):\n",
    "            frame_pad = tf.pad(frames, paddings=[[0,0], [0,0], [K//2,K//2], [K//2,K//2], [0,0]], mode='constant')\n",
    "            for i in range(K):\n",
    "                for j in range(K):\n",
    "                    img_stack.append(frame_pad[:, :, i:i+height, j:j+width,:])\n",
    "            img_stack = tf.stack(img_stack, axis=-1)                 # (bs, N, h, w，color, K*K) \n",
    "        else:\n",
    "            # k_diff = (kernel[index - 1]**2 - kernel[index]**2) // 2\n",
    "            k_diff = (kernel[index-1] - kernel[index]) // 2\n",
    "            k_chosen = []\n",
    "            for i in range(k_diff, kernel[index-1]-k_diff):\n",
    "                k_chosen += [i*kernel[index-1]+j for j in range(k_diff, kernel[index-1]-k_diff)]\n",
    "            # img_stack = img_stack[..., k_diff:-k_diff]\n",
    "            img_stack = tf.convert_to_tensor(img_stack.numpy()[..., k_chosen])\n",
    "        pred_img.append(tf.reduce_sum(tf.math.multiply(core[K], img_stack), axis=-1, keepdims=False))\n",
    "    pred_img = tf.stack(pred_img, axis=0)                           # (nb_kernels, bs, N, h, w, color)\n",
    "    pred_img_i = tf.reduce_mean(pred_img, axis=0, keepdims=False)   # (bs, N, h, w, color)\n",
    "\n",
    "    #pred_img_i += bias\n",
    "\n",
    "    pred_img = tf.reduce_mean(pred_img_i, axis=1, keepdims=False)          # (bs, h, w, color)\n",
    "    return pred_img, pred_img_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "K = 3\n",
    "for step, (batch_test_X, batch_test_Y) in enumerate(test_dataset.take(1)):\n",
    "    '''fetch the patches over every pixel'''\n",
    "    batch_size, N, height, width, color = tf.expand_dims(batch_test_X[...,0], axis=-1).shape \n",
    "    batch_test_X_flatten = []\n",
    "    frame_pad = tf.pad(batch_test_X, paddings=[[0,0], [0,0], [K//2,K//2], [K//2,K//2], [0,0]], mode='constant')\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            batch_test_X_flatten.append(frame_pad[:, :, i:i+height, j:j+width,:])\n",
    "    batch_test_X_flatten = tf.stack(batch_test_X_flatten, axis=-1)       \n",
    "    batch_test_X_flatten = batch_test_X_flatten.numpy().reshape(-1, 9)\n",
    "    print(batch_test_X_flatten.shape)\n",
    "    \n",
    "    '''simulate the patches by the few learned patches and use the corresponding kernels'''\n",
    "    core = kernels[simulate_patches(batch_test_X_flatten, patches)]\n",
    "    print(core.shape)\n",
    "    \n",
    "    core = core.reshape(batch_size, N, height, width, color, -1)\n",
    "    core = dict({3: core}) # use dict\n",
    "    print(core[3].shape)\n",
    "    \n",
    "    pred_test_Y, _ = apply_filtering(batch_test_X, core, kernel_size = [3])\n",
    "    print(pred_test_Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,80))\n",
    "for i in range(16):\n",
    "    plt.subplot(16, 6, 6*i+1)\n",
    "    plt.imshow(batch_test_X[i, ...,0].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('noisy image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+2)\n",
    "    plt.imshow(batch_test_Y[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('ground truth')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+3)\n",
    "    plt.imshow(pred_test_Y[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('recovered image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(16, 6, 6*i+4)\n",
    "#     plt.imshow(pred_test_Y3_clustered[i].numpy().squeeze(), cmap='gray')\n",
    "#     plt.title('recovered image by clustered kernels')\n",
    "#     plt.axis('off')\n",
    "\n",
    "#     plt.subplot(16, 6, 6*i+5)\n",
    "#     plt.imshow(tf.reduce_mean(core[5][i], axis=-1).numpy().squeeze(), cmap='gray')\n",
    "#     plt.title('filter 5x5 {:.3f}'.format(tf.reduce_mean(core[5][i]).numpy().squeeze()))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(16, 6, 6*i+6)\n",
    "#     plt.imshow(tf.reduce_mean(core[7][i], axis=-1).numpy().squeeze(), cmap='gray')\n",
    "#     plt.title('filter 7x7 {:.3f}'.format(tf.reduce_mean(core[7][i]).numpy().squeeze()))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "# plt.savefig('./eval/' + sub_dir + '/kpn3/recovered_images_by_30clustered_kernels.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the neural network to choose the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_test_loss = []\n",
    "for (batch_test_X, batch_test_Y) in test_dataset:\n",
    "    #pred_test_Y = model(batch_test_X)\n",
    "    pred_test_Y, _, core = model(batch_test_X, tf.expand_dims(batch_test_X[...,0], axis=-1))\n",
    "    \n",
    "#     batch_size, N, height, width, color = tf.expand_dims(batch_test_X[...,0], axis=-1).shape \n",
    "#     core, _ = model.kernel_pred._convert_dict(core, batch_size, N, height, width, color)\n",
    "#     test_loss = loss_func(pred_test_Y, batch_test_Y, core)\n",
    "    test_loss = loss_func(pred_test_Y, batch_test_Y)\n",
    "                          \n",
    "    total_test_loss.append(test_loss.numpy())\n",
    "total_test_loss = np.mean(total_test_loss)\n",
    "\n",
    "print(\"Test data loss: {:.3f}\".format(total_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw test figures\n",
    "n_chosen = np.random.randint(test_X_p.shape[0], size = 16)\n",
    "test_x = test_X_p[n_chosen] \n",
    "test_y = test_Y_p[n_chosen] \n",
    "#pred_y = model(test_x)\n",
    "pred_y, _, _ = model(test_x, tf.expand_dims(test_x[...,0], axis=-1))\n",
    "    \n",
    "plt.figure(figsize = (15,5*batch_size))\n",
    "i = 1\n",
    "    \n",
    "for n in range(batch_size):\n",
    "    plt.subplot(batch_size,3,i)\n",
    "    plt.imshow(test_x[n][...,0].squeeze(), cmap='gray')\n",
    "    #plt.title('noise var {:.3f}'.format(test_x[n][...,1].mean()))\n",
    "    plt.axis('off')\n",
    "    i += 1\n",
    "\n",
    "    plt.subplot(batch_size,3,i)\n",
    "    plt.imshow(test_y[n].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    i += 1\n",
    "    \n",
    "    plt.subplot(batch_size,3,i)\n",
    "    plt.imshow(pred_y[n].numpy().squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    i += 1\n",
    "\n",
    "plt.savefig('./results/images/' + sub_dir + '/' + filename + '_' + current_time + '.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def error(x1, x2, mode='mse'):\n",
    "    if mode == 'mse':\n",
    "        return np.mean(np.square(x1-x2))\n",
    "    elif mode == 'mae':\n",
    "        return np.mean(np.abs(x1-x2))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = []\n",
    "test_Y = []\n",
    "pred_Y = []\n",
    "for inputs, target in test_dataset:\n",
    "    test_X.append(inputs.numpy())\n",
    "    test_Y.append(target.numpy())\n",
    "    \n",
    "    #outputs = model(inputs)\n",
    "    outputs, _, _ = model(inputs, tf.expand_dims(inputs[...,0], axis=-1))\n",
    "    pred_Y.append(outputs.numpy())\n",
    "\n",
    "test_X = np.concatenate(test_X, axis=0)\n",
    "test_Y = np.concatenate(test_Y, axis=0)\n",
    "pred_Y = np.concatenate(pred_Y, axis=0)\n",
    "\n",
    "print('Evaluation of ground truth and noised images:')\n",
    "print('psnr:{:.3f}\\tssmi:{:.3f}\\tmse:{:.3f}'.format(psnr(test_X[..., 0].squeeze(), test_Y.squeeze(), data_range=1), \n",
    "                                        ssim(test_X[..., 0].squeeze(), test_Y.squeeze(), data_range=1),\n",
    "                                        error(test_X[..., 0].squeeze(), test_Y.squeeze())))\n",
    "\n",
    "print('\\nEvaluation of recovered images and noised images:')\n",
    "print('psnr:{:.3f}\\tssmi:{:.3f}\\tmse:{:.3f}'.format(psnr(pred_Y, test_Y, data_range=1), \n",
    "                                        ssim(pred_Y.squeeze(), test_Y.squeeze(), data_range=1),\n",
    "                                        error(pred_Y, test_Y)))\n",
    "\n",
    "print('\\nGround Truth:')\n",
    "print('max:{:.3f}\\tmin:{:.3f}\\tmean:{:.3f}'.format(test_Y.max(), test_Y.min(), test_Y.mean()))\n",
    "\n",
    "print('\\nNoised images:')\n",
    "print('max:{:.3f}\\tmin:{:.3f}\\tmean:{:.3f}'.format(test_X[..., 0].max(), test_X[..., 0].min(), test_X.mean()))\n",
    "\n",
    "print('\\nRecoverd images:')\n",
    "print('max:{:.3f}\\tmin:{:.3f}\\tmean:{:.3f}'.format(pred_Y.max(), pred_Y.min(), pred_Y.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2gpu",
   "language": "python",
   "name": "tf2gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
