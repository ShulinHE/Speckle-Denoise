{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:45:53.264352Z",
     "start_time": "2020-05-19T18:45:47.165668Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from data import read_data\n",
    "from utils import add_noise_est, normalize, add_noise\n",
    "from model_noise_est import FCN\n",
    "\n",
    "gpu_ok = tf.test.is_gpu_available()\n",
    "print(\"tf version:\", tf.__version__)\n",
    "print(\"use GPU:\", gpu_ok)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T14:11:31.947499Z",
     "start_time": "2020-05-12T14:11:31.663258Z"
    }
   },
   "source": [
    "# 避免显卡显存小而报错，显存自适应分配\n",
    "physical_devices=tf.config.experimental.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0],True)\n",
    "\n",
    "# 给显存分配几个子虚拟内存\n",
    "tf.config.experimental.set_virtual_device_configuration(\n",
    "    physical_devices[0],\n",
    "    [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=2048)])\n",
    "\n",
    "# 发现容易报错This is probably because cuDNN failed to initialize，是显存还是不够的原因"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:45:53.334166Z",
     "start_time": "2020-05-19T18:45:53.268340Z"
    }
   },
   "outputs": [],
   "source": [
    "ims = read_data('imagenet')\n",
    "\n",
    "N_ims, h, w, _ = ims.shape\n",
    "ims = ims[:N_ims].astype(np.float32)\n",
    "ims_noise = ims_noise[:N_ims].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for noise estimation map, there should be different noise levels\n",
    "ims_noise = []\n",
    "ims_noise_with_est = []\n",
    "variances = np.arange(1,11) * 5e-4\n",
    "# variances = [80e-4]\n",
    "ims_split = np.array_split(ims, len(variances))\n",
    "for i, var in enumerate(variances):\n",
    "    ims_noise.append(normalize(add_noise(ims_split[i], mean=0, var=var, n_type='gaussian')))\n",
    "    ims_noise_with_est.append(add_noise_est(ims_noise[i], var = var))\n",
    "ims_noise = np.concatenate(ims_noise)\n",
    "ims_noise_with_est = np.concatenate(ims_noise_with_est)\n",
    "\n",
    "print(ims.shape)\n",
    "print(ims_noise.shape)\n",
    "print(ims_noise_with_est.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:45:53.339151Z",
     "start_time": "2020-05-19T18:45:53.335161Z"
    }
   },
   "outputs": [],
   "source": [
    "# training hyperparameters\n",
    "batch_size = 16\n",
    "lr = 3e-4\n",
    "epochs = 80\n",
    "test_size = 0.1\n",
    "training_steps = int(epochs*N_ims*(1-test_size)/batch_size)\n",
    "display_step = int(training_steps/epochs*0.2)\n",
    "\n",
    "print(training_steps)\n",
    "print(display_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:46:04.135820Z",
     "start_time": "2020-05-19T18:46:04.060007Z"
    }
   },
   "outputs": [],
   "source": [
    "# train test split\n",
    "train_X, train_Y = ims_noise_with_est[...,0][...,np.newaxis], ims_noise_with_est[...,1][...,np.newaxis]\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(train_X, train_Y, test_size=test_size, random_state=42)\n",
    "\n",
    "print('Training X: ', train_X.shape, train_X.dtype)\n",
    "print('Training Y: ', train_Y.shape, train_Y.dtype)\n",
    "print('Testing X: ', test_X.shape, test_X.dtype)\n",
    "print('Testing Y: ', test_Y.shape, test_Y.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:46:04.650555Z",
     "start_time": "2020-05-19T18:46:04.635597Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use tf.data API to shuffle and batch data.\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X,train_Y))\n",
    "train_dataset = train_dataset.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_X,test_Y))\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:46:05.180157Z",
     "start_time": "2020-05-19T18:46:05.154215Z"
    }
   },
   "outputs": [],
   "source": [
    "# model\n",
    "model = FCN(color = False, channels = [16, 16, 32, 32, 64, 64, 32, 32, 16, 16],\n",
    "            channel_att=False, spatial_att=False, use_bias = True)\n",
    "\n",
    "load_model = True\n",
    "if load_model:\n",
    "    model.load_weights(filepath = \"model_weights/model_noise_est.ckpt\")\n",
    "    \n",
    "# print(np.sum([np.prod(v.get_shape().as_list()) for v in model.trainable_variables]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:46:07.533165Z",
     "start_time": "2020-05-19T18:46:07.529176Z"
    }
   },
   "outputs": [],
   "source": [
    "# optimizer\n",
    "optimizer = tf.keras.optimizers.Adam(lr)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9, nesterov=True, decay=1e-6)\n",
    "\n",
    "# loss func\n",
    "#loss_func = tf.keras.losses.MeanAbsoluteError()\n",
    "loss_func = tf.keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:46:09.482949Z",
     "start_time": "2020-05-19T18:46:09.477962Z"
    }
   },
   "outputs": [],
   "source": [
    "# optimization process\n",
    "def lr_fn(step, cur_lr):\n",
    "    '''exponetial'''\n",
    "    next_epoch = step * batch_size // int(N_ims*(1-test_size)) - (step-1) * batch_size // int(N_ims*(1-test_size))\n",
    "    return cur_lr * (0.95**next_epoch)\n",
    "\n",
    "def run_optimization(step, train_X, train_Y):\n",
    "    with tf.GradientTape() as g:\n",
    "        pred_Y = model(train_X) \n",
    "        loss = loss_func(pred_Y, train_Y)\n",
    "    \n",
    "    gradients = g.gradient(loss, model.trainable_variables)\n",
    "    optimizer.learning_rate = lr_fn(step, optimizer.learning_rate.numpy())\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-19T18:46:10.483373Z",
     "start_time": "2020-05-19T18:46:10.479384Z"
    }
   },
   "outputs": [],
   "source": [
    "# 发现用tensorboard summary会让速度变得很慢很慢\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "test_steps = []\n",
    "lrs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-20T02:11:15.684Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_train_loss = total_train = 0\n",
    "for step, (batch_X, batch_Y) in enumerate(train_dataset.take(training_steps), start = 1):\n",
    "    train_loss = run_optimization(step, batch_X, batch_Y)\n",
    "    \n",
    "    mean_train_loss +=  train_loss.numpy()\n",
    "    total_train += 1\n",
    "    train_losses.append(train_loss.numpy())\n",
    "    lrs.append(optimizer.lr.numpy())\n",
    "    \n",
    "    if step % display_step == 0:\n",
    "        mean_test_loss = total_test = 0\n",
    "        for (batch_test_X, batch_test_Y) in test_dataset:\n",
    "            pred_test_Y = model(batch_test_X)\n",
    "            test_loss = loss_func(pred_test_Y, batch_test_Y)\n",
    "            \n",
    "            mean_test_loss += test_loss.numpy()\n",
    "            total_test += 1\n",
    "        \n",
    "        mean_test_loss /= total_test\n",
    "        mean_train_loss /= total_train\n",
    "        test_losses.append(mean_test_loss)\n",
    "        test_steps.append(step)\n",
    "\n",
    "        print(\"step: {:3d}/{:3d} || train loss: {:.5f} || test loss: {:.5f}\"\n",
    "              .format(step, training_steps, mean_train_loss, mean_test_loss))\n",
    "        \n",
    "        mean_train_loss = total_train = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_dir = './logs/'\n",
    "filename = 'model_noise_est'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:09:16.160563Z",
     "start_time": "2020-05-20T02:09:15.803491Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,5))\n",
    "plt.subplot(131)\n",
    "plt.plot(range(training_steps)[100:], train_losses[100:])\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('value')\n",
    "plt.title('training loss')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(test_steps[1:], test_losses[1:])\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('value')\n",
    "plt.title('test loss')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(range(training_steps), lrs)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('value')\n",
    "plt.title('learning rate')\n",
    "\n",
    "plt.savefig(log_dir+filename+'_'+current_time+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:09:40.099066Z",
     "start_time": "2020-05-20T02:09:24.427994Z"
    }
   },
   "outputs": [],
   "source": [
    "total_test_loss = []\n",
    "for (batch_test_X, batch_test_Y) in test_dataset:\n",
    "    pred_test_Y = model(batch_test_X)\n",
    "    test_loss = loss_func(pred_test_Y, batch_test_Y)\n",
    "    total_test_loss.append(test_loss.numpy())\n",
    "total_test_loss = np.mean(total_test_loss)\n",
    "\n",
    "print(\"Test data loss: {:.5f}\".format(total_test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:09:47.021544Z",
     "start_time": "2020-05-20T02:09:40.100063Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw test figures\n",
    "test_x = test_X[:batch_size] \n",
    "test_y = test_Y[:batch_size] \n",
    "pred_y = model(test_x)\n",
    "    \n",
    "plt.figure(figsize = (15,5*batch_size))\n",
    "i = 1\n",
    "    \n",
    "for n in range(batch_size):\n",
    "    plt.subplot(batch_size,3,i)\n",
    "    plt.imshow(test_x[n].squeeze(), cmap='gray')\n",
    "    plt.title('noise var {:.3f}'.format(test_y[n].mean()))\n",
    "    plt.axis('off')\n",
    "    i += 1\n",
    "\n",
    "    plt.subplot(batch_size,3,i)\n",
    "    plt.imshow(test_y[n].squeeze(), cmap='gray')\n",
    "    plt.axis('off')\n",
    "    i += 1\n",
    "    \n",
    "    plt.subplot(batch_size,3,i)\n",
    "    plt.imshow(pred_y[n].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('est var {:.3f}'.format(pred_y[n].numpy().mean()))\n",
    "    plt.axis('off')\n",
    "    i += 1\n",
    "\n",
    "plt.savefig('./results/images/'+filename+'_'+current_time+'.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T02:09:47.037502Z",
     "start_time": "2020-05-20T02:09:47.022543Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "\n",
    "def error(x1, x2, mode='mse'):\n",
    "    if mode == 'mse':\n",
    "        return np.mean(np.square(x1-x2))\n",
    "    elif mode == 'mae':\n",
    "        return np.mean(np.abs(x1-x2))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = []\n",
    "test_Y = []\n",
    "pred_Y = []\n",
    "for inputs, target in test_dataset:\n",
    "    test_X.append(inputs.numpy())\n",
    "    test_Y.append(target.numpy())\n",
    "    \n",
    "    outputs = model(inputs)\n",
    "    pred_Y.append(outputs.numpy())\n",
    "\n",
    "test_X = np.concatenate(test_X, axis=0)\n",
    "test_Y = np.concatenate(test_Y, axis=0)\n",
    "pred_Y = np.concatenate(pred_Y, axis=0)\n",
    "\n",
    "print('Evaluation of ground truth and noised images:')\n",
    "print('psnr:{:.3f}\\tssmi:{:.3f}\\tmse:{:.3f}'.format(psnr(test_X[..., 0].squeeze(), test_Y.squeeze(), data_range=1), \n",
    "                                        ssim(test_X[..., 0].squeeze(), test_Y.squeeze(), data_range=1),\n",
    "                                        error(test_X, test_Y)))\n",
    "\n",
    "print('\\nEvaluation of recovered images and noised images:')\n",
    "print('psnr:{:.3f}\\tssmi:{:.3f}\\tmse:{:.3f}'.format(psnr(pred_Y, test_Y, data_range=1), \n",
    "                                        ssim(pred_Y.squeeze(), test_Y.squeeze(), data_range=1),\n",
    "                                        error(pred_Y, test_Y)))\n",
    "\n",
    "print('\\nGround Truth:')\n",
    "print('max:{:.3f}\\tmin:{:.3f}\\tmean:{:.3f}'.format(test_Y.max(), test_Y.min(), test_Y.mean()))\n",
    "\n",
    "print('\\nNoised images:')\n",
    "print('max:{:.3f}\\tmin:{:.3f}\\tmean:{:.3f}'.format(test_X[..., 0].max(), test_X[..., 0].min(), test_X.mean()))\n",
    "\n",
    "print('\\nRecoverd images:')\n",
    "print('max:{:.3f}\\tmin:{:.3f}\\tmean:{:.3f}'.format(pred_Y.max(), pred_Y.min(), pred_Y.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T14:54:41.411260Z",
     "start_time": "2020-05-11T14:54:41.408269Z"
    }
   },
   "outputs": [],
   "source": [
    "# draw loss\n",
    "# 1.在命令行输入：\n",
    "# python -m tensorboard.main --logdir logs\n",
    "# 2.在浏览器输入\n",
    "# http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-14T06:54:09.739436Z",
     "start_time": "2020-05-14T06:54:09.554927Z"
    }
   },
   "outputs": [],
   "source": [
    "# Save TF model.\n",
    "model.save_weights(filepath=\"model_weights/\"+filename+\".ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-13T12:12:26.597073Z",
     "start_time": "2020-05-13T12:12:26.467421Z"
    }
   },
   "outputs": [],
   "source": [
    "for i, v in enumerate(model.trainable_variables):\n",
    "    print(i, v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2gpu",
   "language": "python",
   "name": "tf2gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
