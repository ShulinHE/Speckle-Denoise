{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-03T14:54:01.757258Z",
     "start_time": "2020-06-03T14:53:56.223999Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from data import read_data\n",
    "from utils import add_noise_est, normalize, add_noise, squeeze_patches, read_div2k_data\n",
    "\n",
    "#from model_global_dfn import GDFN\n",
    "from model_baseline import Unet\n",
    "from model_mwcnn import MWCNN\n",
    "from model_mwkpn import MWKPN\n",
    "from model_kpn import KPN, LossFunc, LossBasic\n",
    "\n",
    "gpu_ok = tf.test.is_gpu_available()\n",
    "print(\"tf version:\", tf.__version__)\n",
    "print(\"use GPU:\", gpu_ok)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse de l'influence de l'ondelette - KPN - div2k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''préparation des données'''\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "(train_X_p, train_Y_p), (test_X_p, test_Y_p) = read_data('div2k')\n",
    "N_ims= len(train_X_p)\n",
    "\n",
    "train_X_p, label_train_X_p = squeeze_patches(train_X_p)\n",
    "train_Y_p, label_train_Y_p = squeeze_patches(train_Y_p)\n",
    "test_X_p, label_test_X_p = squeeze_patches(test_X_p)\n",
    "test_Y_p, label_test_Y_p = squeeze_patches(test_Y_p)\n",
    "\n",
    "train_X_p = train_X_p[:,np.newaxis,...]\n",
    "train_Y_p = train_Y_p[...,np.newaxis]\n",
    "test_X_p = test_X_p[:,np.newaxis,...]\n",
    "test_Y_p = test_Y_p[...,np.newaxis]\n",
    "\n",
    "print('\\nTrain data:')\n",
    "print('train_X_p:',train_X_p.shape)\n",
    "print('train_Y_p:',train_Y_p.shape)\n",
    "\n",
    "print('\\nTest data:')\n",
    "print('test_X_p:',test_X_p.shape)\n",
    "print('test_Y_p:',test_Y_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_noise_map = False   # if True, concatenate a noise map to the input\n",
    "#use_noise_est = False   # if True, use a model to estimate noise map, if False, use known info\n",
    "\n",
    "if not use_noise_map:\n",
    "    train_X_p = train_X_p[...,0][..., np.newaxis]\n",
    "    test_X_p = test_X_p[...,0][..., np.newaxis]\n",
    "    \n",
    "print('Train data:')\n",
    "print('train_X_p:',train_X_p.shape)\n",
    "print('train_Y_p:',train_Y_p.shape)\n",
    "\n",
    "print('\\nTest data:')\n",
    "print('test_X_p:',test_X_p.shape)\n",
    "print('test_Y_p:',test_Y_p.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use tf.data API to shuffle and batch data.\n",
    "batch_size = 16\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_X_p,train_Y_p))\n",
    "train_dataset = train_dataset.repeat().shuffle(5000).batch(batch_size).prefetch(1)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_X_p,test_Y_p))\n",
    "test_dataset = test_dataset.batch(batch_size).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KPN(color=False, burst_length=1, blind_est=True, sep_conv=False, kernel_size=[3],\n",
    "            channel_att=False, spatial_att=True, core_bias=True, use_bias=True)\n",
    "#model = MWKPN(color=False, burst_length=1, blind_est=True, sep_conv=False, kernel_size=[3,5,7],\n",
    "#             channel_att=False, spatial_att=True, core_bias=True, use_bias=True)\n",
    "\n",
    "sub_dir = 'transfer_to_div2k'\n",
    "filename = 'kpn_ks3_satt_bias_combinedsymetricloss'\n",
    "\n",
    "load_model = True\n",
    "if load_model:\n",
    "    model.load_weights(filepath = \"model_weights/\" + sub_dir + '/' + filename + \".ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, (batch_test_X, batch_test_Y) in enumerate(test_dataset.take(1)):\n",
    "    pred_test_Y, _, core = model(batch_test_X, tf.expand_dims(batch_test_X[...,0], axis=-1))\n",
    "    \n",
    "    batch_size, N, height, width, color = tf.expand_dims(batch_test_X[...,0], axis=-1).shape \n",
    "    core, bias = model.kernel_pred._convert_dict(core, batch_size, N, height, width, color)\n",
    "    #print(core[3].shape, core[5].shape, core[7].shape, bias.shape)\n",
    "    \n",
    "    core3 = tf.reshape(tf.reduce_mean(tf.squeeze(core[3]), [0,1,2], keepdims=False), [3,3]).numpy()\n",
    "    #core5 = tf.reshape(tf.reduce_mean(tf.squeeze(core[5]), [0,1,2], keepdims=False), [5,5]).numpy()\n",
    "    #core7 = tf.reshape(tf.reduce_mean(tf.squeeze(core[7]), [0,1,2], keepdims=False), [7,7]).numpy()\n",
    "    print(core3.shape)\n",
    "    #print(core5.shape)\n",
    "    #print(core7.shape)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (30,10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(core3, cmap='gray')\n",
    "plt.title('3x3 filter')\n",
    "\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.imshow(core5, cmap='gray')\n",
    "# plt.title('5x5 filter')\n",
    "\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.imshow(core7, cmap='gray')\n",
    "# plt.title('7x7 filter')\n",
    "    \n",
    "#plt.savefig('./eval/' + sub_dir + '/kpn357_symetricloss/kernels.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyse the filters applied to the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for step, (batch_test_X, batch_test_Y) in enumerate(test_dataset.take(1)):\n",
    "    pred_test_Y, _, core = model(batch_test_X, tf.expand_dims(batch_test_X[...,0], axis=-1))\n",
    "    \n",
    "    batch_size, N, height, width, color = tf.expand_dims(batch_test_X[...,0], axis=-1).shape \n",
    "    core, bias = model.kernel_pred._convert_dict(core, batch_size, N, height, width, color)\n",
    "    #print(core[3].shape, core[5].shape, core[7].shape, bias.shape)\n",
    "    print(pred_test_Y.shape)\n",
    "    print(core[3].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10, 10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3, 3, i+1)\n",
    "    plt.imshow(core[3].numpy().mean(axis=0).squeeze()[...,i], cmap='gray')\n",
    "    plt.title('plan {} mean {}'.format(i+1, core[3].numpy().squeeze()[...,i].mean()))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,80))\n",
    "for i in range(16):\n",
    "    plt.subplot(16, 6, 6*i+1)\n",
    "    plt.imshow(batch_test_X[i, ...,0].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('noisy image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+2)\n",
    "    plt.imshow(batch_test_Y[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('ground truth')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+3)\n",
    "    plt.imshow(pred_test_Y[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('recovered image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+4)\n",
    "    plt.imshow(tf.reduce_mean(core[3][i], axis=-1).numpy().squeeze(), cmap='gray')\n",
    "    plt.title('filter 3x3 {:.3f}'.format(tf.reduce_mean(core[3][i]).numpy().squeeze()))\n",
    "    plt.axis('off')\n",
    "\n",
    "#     plt.subplot(16, 6, 6*i+5)\n",
    "#     plt.imshow(tf.reduce_mean(core[5][i], axis=-1).numpy().squeeze(), cmap='gray')\n",
    "#     plt.title('filter 5x5 {:.3f}'.format(tf.reduce_mean(core[5][i]).numpy().squeeze()))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(16, 6, 6*i+6)\n",
    "#     plt.imshow(tf.reduce_mean(core[7][i], axis=-1).numpy().squeeze(), cmap='gray')\n",
    "#     plt.title('filter 7x7 {:.3f}'.format(tf.reduce_mean(core[7][i]).numpy().squeeze()))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#plt.savefig('./eval/' + sub_dir + '/kpn357_symetricloss/recovered_images.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = 0.1*np.arange(11)\n",
    "\n",
    "plt.figure(figsize = (30,10))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(np.abs(core[3].numpy().flatten().squeeze()), bins=bins) \n",
    "plt.title('filter 3x3 {:.3f}'.format(tf.reduce_mean(core[3][i]).numpy().squeeze()))\n",
    "plt.ylabel('number')\n",
    "plt.xlabel('mean value')\n",
    "\n",
    "# plt.subplot(1, 3, 2)\n",
    "# plt.hist(np.abs(core[5].numpy().flatten().squeeze()), bins=bins) \n",
    "# plt.title('filter 5x5 {:.3f}'.format(tf.reduce_mean(core[5][i]).numpy().squeeze()))\n",
    "# plt.ylabel('number')\n",
    "# plt.xlabel('value')\n",
    "\n",
    "# plt.subplot(1, 3, 3)\n",
    "# plt.hist(np.abs(core[7].numpy().flatten().squeeze()), bins=bins) \n",
    "# plt.title('filter 7x7 {:.3f}'.format(tf.reduce_mean(core[7][i]).numpy().squeeze()))\n",
    "# plt.ylabel('number')\n",
    "# plt.xlabel('value')\n",
    "    \n",
    "# plt.savefig('./eval/' + sub_dir + '/kpn357_symetricloss/filter_vdistribution.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (90,160))\n",
    "for i in range(16):\n",
    "    for j in range(9):\n",
    "        plt.subplot(16, 9, 9*i+j+1)\n",
    "        plt.imshow(core[3][i,...,j].numpy().squeeze(), cmap='gray')\n",
    "        plt.title(\"mean {:.3f}\".format(core[3][i,...,j].numpy().squeeze().mean()), fontsize=40)\n",
    "        #plt.title('noisy image')\n",
    "        plt.axis('off')\n",
    "    \n",
    "#plt.savefig('./eval/' + sub_dir + '/kpn3/filter_3x3.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,10))\n",
    "\n",
    "def normalize(im):\n",
    "    return (im - im.min())/(im.max()-im.min())\n",
    "    \n",
    "plt.subplot(1, 3, 1)\n",
    "n = 4\n",
    "plt.imshow(core[3][0,...,n].numpy().squeeze()*batch_test_X[0, ...,0].numpy().squeeze(), cmap='gray')\n",
    "plt.title('filtered image No.{} std {:.3f}'.format(n+1, normalize((core[3][0,...,n].numpy().squeeze()*batch_test_X[0, ...,0].numpy().squeeze())).std()))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "n = 0\n",
    "plt.imshow(core[3][0,...,n].numpy().squeeze()*batch_test_X[0, ...,0].numpy().squeeze(), cmap='gray')\n",
    "plt.title('filtered image No.{} std {:.3f}'.format(n+1, normalize((core[3][0,...,n].numpy().squeeze()*batch_test_X[0, ...,0].numpy().squeeze())).std()))\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(batch_test_X[0, ...,0].numpy().squeeze(), cmap='gray')\n",
    "plt.title('final filtered image std {:.3f}'.format(normalize((core[3][0,...,n].numpy().squeeze()*batch_test_X[0, ...,0].numpy().squeeze())).std()))\n",
    "plt.axis('off')\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(ims):\n",
    "    ims_new = []\n",
    "    for im in ims:\n",
    "        im = (im-im.min())/(im.max()-im.min())\n",
    "        ims_new.append(im)\n",
    "    return np.array(ims_new)\n",
    "\n",
    "\n",
    "N_rate = 0.02\n",
    "\n",
    "core3_selected_all = []\n",
    "for step, (batch_test_X, batch_test_Y) in enumerate(test_dataset):\n",
    "    pred_test_Y, _, core = model(batch_test_X, tf.expand_dims(batch_test_X[...,0], axis=-1))\n",
    "    \n",
    "    batch_size, N, height, width, color = tf.expand_dims(batch_test_X[...,0], axis=-1).shape \n",
    "    core, bias = model.kernel_pred._convert_dict(core, batch_size, N, height, width, color)\n",
    "    \n",
    "    core[3] = tf.reshape(core[3], [-1,9]).numpy()\n",
    "    core3_ind = np.random.randint(core[3].shape[0], size=int(N_rate*core[3].shape[0]))\n",
    "    core3_selected = core[3][core3_ind]\n",
    "    \n",
    "    core3_selected_all.append(core3_selected)\n",
    "    \n",
    "core3_selected_all = np.concatenate(core3_selected_all, axis=0)\n",
    "#core3_selected_all = normalize(core3_selected_all)\n",
    "print(core3_selected_all.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the variance of kernels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds_all = []\n",
    "for k in core3_selected_all:\n",
    "    std = np.std(k)\n",
    "    stds_all.append(std)\n",
    "stds_all = np.array(stds_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_bins(vmin, vmax, n):\n",
    "    vdiff = vmax - vmin\n",
    "    bins = [vmin]\n",
    "    for i in range(n):\n",
    "        bins.append(vmin+(i+1)/n*vdiff)\n",
    "    return bins\n",
    "\n",
    "bins = create_bins(stds_all.min(), stds_all.max(), 10)\n",
    "plt.figure()\n",
    "plt.hist(stds_all, bins=bins)\n",
    "plt.xlabel('standard deviation')\n",
    "plt.ylabel('numbers of kernels')\n",
    "#plt.savefig('./eval/' + sub_dir + '/kpn3/kernels_std.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "for i in range(16):\n",
    "    ind = np.random.randint(core3_selected_all.shape[0], size=1)[0]\n",
    "    plt.subplot(4,4,i+1)\n",
    "    plt.imshow(core3_selected_all[ind].reshape(3,3), cmap='gray')\n",
    "    plt.title(ind)\n",
    "    plt.axis('off')\n",
    "#plt.savefig('./eval/' + sub_dir + '/kpn3/kernels_exemples.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "n_clusters = 3\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "y_preds = kmeans.fit_predict(core3_selected_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(ims):\n",
    "    ims_new = []\n",
    "    for im in ims:\n",
    "        im = (im-im.min())/(im.max()-im.min())\n",
    "        ims_new.append(im)\n",
    "    return np.array(ims_new)\n",
    "\n",
    "normalize(kmeans.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (5*n_clusters,5))\n",
    "for i in range(n_clusters):\n",
    "    plt.subplot(1,n_clusters,i+1)\n",
    "    plt.imshow(kmeans.cluster_centers_[i].reshape(3,3), cmap='gray')\n",
    "    plt.axis('off')\n",
    "#plt.savefig('./eval/' + sub_dir + '/kpn3/kernels_kmeans3.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA/ICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA, FastICA\n",
    "from sklearn.mixture import GaussianMixture as GMM\n",
    "\n",
    "pca = PCA(n_components = 2)\n",
    "pca.fit(core3_selected_all)\n",
    "\n",
    "core3_proj = pca.transform(core3_selected_all)\n",
    "print(core3_proj.shape)\n",
    "\n",
    "'''for color labels'''\n",
    "n_clusters = 10\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "#kmeans = GMM(n_components=n_clusters, random_state=0)\n",
    "kmeans.fit(core3_selected_all)\n",
    "\n",
    "plt.figure(figsize = (8,8))\n",
    "plt.scatter(core3_proj[:,0], core3_proj[:,1], edgecolor='none', c = kmeans.predict(core3_selected_all), alpha = 0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
    "#plt.savefig('./eval/' + sub_dir + '/kpn3/kernels_pca.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = GMM(n_components=n_clusters, random_state=0)\n",
    "kmeans.fit(core3_selected_all)\n",
    "\n",
    "kmeans.predict(core3_selected_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA(n_components = 9)\n",
    "pca_full.fit(core3_selected_all)\n",
    "cum_var = np.cumsum(pca_full.explained_variance_)\n",
    "\n",
    "plt.figure()\n",
    "axis_x = np.arange(1,10)\n",
    "plt.plot(axis_x, cum_var)\n",
    "plt.xlabel('reduced dimensionality')\n",
    "plt.ylabel('cumulative variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.linalg import svd\n",
    "\n",
    "U,s,Vh = svd(core3_selected_all, full_matrices=False)\n",
    "print(U.shape, s.shape, Vh.shape)\n",
    "print(s)\n",
    "\n",
    "K = 1\n",
    "U_new = U[:, :K]\n",
    "Vh_new = Vh[:K, :]\n",
    "s_new = np.diag(s[:K])\n",
    "print()\n",
    "print(U_new.shape, s_new.shape, Vh_new.shape)\n",
    "\n",
    "core3_selected_all_svd = U_new.dot(s_new).dot(Vh_new)\n",
    "print(core3_selected_all_svd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "for i in range(9):\n",
    "    plt.subplot(3,3,i+1)\n",
    "    plt.imshow(U[:,i][:,np.newaxis].dot(Vh[i,:][np.newaxis,:]).mean(axis=0).reshape(3,3), cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyse the clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 5\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "\n",
    "core3_all = []\n",
    "for step, (batch_test_X, batch_test_Y) in enumerate(test_dataset.take(1)):\n",
    "    pred_test_Y, _, core = model(batch_test_X, tf.expand_dims(batch_test_X[...,0], axis=-1))\n",
    "    \n",
    "    batch_size, N, height, width, color = tf.expand_dims(batch_test_X[...,0], axis=-1).shape \n",
    "    core, bias = model.kernel_pred._convert_dict(core, batch_size, N, height, width, color)\n",
    "    \n",
    "    core3 = tf.reshape(core[3], [-1,9]).numpy()\n",
    "    core3_all.append(core3)\n",
    "\n",
    "core3_all = np.concatenate(core3_all, axis=0)\n",
    "kmeans.fit(core3_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_test_X_flatten = []\n",
    "K = 3\n",
    "frame_pad = tf.pad(batch_test_X, paddings=[[0,0], [0,0], [K//2,K//2], [K//2,K//2], [0,0]], mode='constant')\n",
    "for i in range(K):\n",
    "    for j in range(K):\n",
    "        batch_test_X_flatten.append(frame_pad[:, :, i:i+height, j:j+width,:])\n",
    "batch_test_X_flatten = tf.stack(batch_test_X_flatten, axis=-1)       \n",
    "print(batch_test_X_flatten.shape)\n",
    "\n",
    "batch_test_X_flatten = batch_test_X_flatten.numpy().reshape(-1, 9)\n",
    "print(batch_test_X_flatten.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_all = dict()\n",
    "for i in range(n_clusters):\n",
    "    test_all[i] = batch_test_X_flatten[np.where(kmeans.labels_==i)[0]].mean(axis=0).reshape(3,3)\n",
    "\n",
    "\n",
    "plt.figure(figsize = (10*n_clusters,20))\n",
    "for i in range(n_clusters):\n",
    "    plt.subplot(2, n_clusters, i+1)\n",
    "    plt.imshow(test_all[i].reshape(3,3), cmap='gray')\n",
    "    plt.title('mean patch for label {}, mean {:.3f}, std {:.3f}'.format(i, test_all[i].mean(), test_all[i].std()), fontsize=25)\n",
    "    plt.axis('off')\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    plt.subplot(2, n_clusters, i+n_clusters+1)\n",
    "    plt.imshow(kmeans.cluster_centers_[i].reshape(3,3), cmap='gray')\n",
    "    plt.title('mean kernel for label {}, mean {:.3f}, std {:.3f}'.format(i, kmeans.cluster_centers_[i].mean(), kmeans.cluster_centers_[i].std()), fontsize=25)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_exemples = 100\n",
    "print(np.where(kmeans.labels_==0)[0].shape, np.where(kmeans.labels_==1)[0].shape, np.where(kmeans.labels_==2)[0].shape)\n",
    "\n",
    "test_all0 = batch_test_X_flatten[np.where(kmeans.labels_==0)[0][:num_exemples]]\n",
    "test_all1 = batch_test_X_flatten[np.where(kmeans.labels_==1)[0][:num_exemples]]\n",
    "test_all2 = batch_test_X_flatten[np.where(kmeans.labels_==2)[0][:num_exemples]]\n",
    "\n",
    "kernel_all0 = core3_all[np.where(kmeans.labels_==0)[0][:num_exemples]]\n",
    "kernel_all1 = core3_all[np.where(kmeans.labels_==1)[0][:num_exemples]]\n",
    "kernel_all2 = core3_all[np.where(kmeans.labels_==2)[0][:num_exemples]]\n",
    "\n",
    "plt.figure(figsize = (20*3,100))\n",
    "for i in range(10):\n",
    "    plt.subplot(10, 6, 6*i+1)\n",
    "    plt.imshow(test_all0[i].reshape(3,3), cmap='gray')\n",
    "    plt.title('test(label0)', fontsize=25)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(10, 6, 6*i+2)\n",
    "    plt.imshow(kernel_all0[i].reshape(3,3), cmap='gray')\n",
    "    plt.title('kernel(label0)', fontsize=25)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(10, 6, 6*i+3)\n",
    "    plt.imshow(test_all1[i].reshape(3,3), cmap='gray')\n",
    "    plt.title('test(label1)', fontsize=25)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(10, 6, 6*i+4)\n",
    "    plt.imshow(kernel_all1[i].reshape(3,3), cmap='gray')\n",
    "    plt.title('kernel(label1)', fontsize=25)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(10, 6, 6*i+5)\n",
    "    plt.imshow(test_all1[i].reshape(3,3), cmap='gray')\n",
    "    plt.title('test(label2)', fontsize=25)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(10, 6, 6*i+6)\n",
    "    plt.imshow(kernel_all2[i].reshape(3,3), cmap='gray')\n",
    "    plt.title('kernel(label2)', fontsize=25)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster the kernels by the direction of their corresponding patches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.filters import sobel_h, sobel_v\n",
    "from skimage.util import pad\n",
    "from skimage.feature import hog\n",
    "\n",
    "def vote_dir(im):\n",
    "    im = pad(im.reshape(3,3), (1,1), 'edge')\n",
    "    grad_h = sobel_h(im) # tendence up down\n",
    "    grad_v = sobel_v(im) # tendence left right\n",
    "    \n",
    "    grad_h = grad_h[1:-1,1:-1].flatten()\n",
    "    grad_v = grad_v[1:-1,1:-1].flatten()\n",
    "    im = im[1:-1,1:-1].flatten()\n",
    "    \n",
    "    vote = np.zeros((8))\n",
    "    for i in range(9):\n",
    "        if grad_h[i] > 0 and grad_v[i] > 0:\n",
    "            if grad_h[i] > grad_v[i]:\n",
    "                vote[0] += 1\n",
    "            else:\n",
    "                vote[1] += 1\n",
    "        elif grad_h[i] < 0 and grad_v[i] > 0:\n",
    "            if abs(grad_h[i]) < grad_v[i]:\n",
    "                vote[2] += 1\n",
    "            else:\n",
    "                vote[3] += 1\n",
    "        elif grad_h[i] < 0 and grad_v[i] < 0:\n",
    "            if grad_h[i] < grad_v[i]:\n",
    "                vote[4] += 1\n",
    "            else:\n",
    "                vote[5] += 1\n",
    "        else:\n",
    "            if grad_h[i] < abs(grad_v[i]):\n",
    "                vote[6] += 1\n",
    "            else:\n",
    "                vote[7] += 1\n",
    "    return vote\n",
    "\n",
    "def ims_dir(ims):\n",
    "    dirs = []\n",
    "    for im in ims:\n",
    "        vote = vote_dir(im)\n",
    "        cur_dir = np.argmax(vote)\n",
    "        dirs.append(cur_dir)\n",
    "    return np.array(dirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = ims_dir(batch_test_X_flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirs = np.array(dirs)\n",
    "batch_test_X_flatten_by_dir = dict()\n",
    "core3_all_by_dir = dict()\n",
    "for i in range(8):\n",
    "    batch_test_X_flatten_by_dir[i] = batch_test_X_flatten[np.where(dirs==i)[0]]\n",
    "    core3_all_by_dir[i] = core3_all[np.where(dirs==i)[0]]\n",
    "    \n",
    "plt.figure(figsize=(40,10))\n",
    "for i in range(8):\n",
    "    plt.subplot(2,8,i+1)\n",
    "    plt.imshow(batch_test_X_flatten_by_dir[i].mean(axis=0).reshape(3,3), cmap='gray')\n",
    "    plt.title('patch dir {}'.format(i))\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,8,i+9)\n",
    "    plt.imshow(core3_all_by_dir[i].mean(axis=0).reshape(3,3), cmap='gray')\n",
    "    plt.title('kernel dir {}'.format(i))\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "core3_all_by_dir[2].mean(axis=0).reshape(3,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' test and visualize '''\n",
    "p = pad(batch_test_X_flatten[1].reshape(3,3), (1,1), 'edge')\n",
    "#fd, hog_im = hog(p, orientations = 8, pixels_per_cell = (1,1), cells_per_block = (1,1), visualize = True) \n",
    "#Hrr, Hrc, Hcc = hessian_matrix(p, sigma=0.1, mode='constant')\n",
    "grad_h = sobel_h(p) # tendence up down\n",
    "grad_v = sobel_v(p) # tendence left right\n",
    "# Wh = grad_h**2 - grad_v**2\n",
    "# Wv = 2*grad_h*grad_v\n",
    "# Wh = Wh[1:-1,1:-1]\n",
    "# Wv = Wv[1:-1,1:-1]\n",
    "grad_h = grad_h[1:-1,1:-1]\n",
    "grad_v = grad_v[1:-1,1:-1]\n",
    "p = p[1:-1,1:-1]\n",
    "\n",
    "plt.figure(figsize=(20,5))\n",
    "plt.subplot(141)\n",
    "plt.imshow(p, cmap='gray')\n",
    "plt.title('patch')\n",
    "plt.subplot(142)\n",
    "plt.imshow(grad_h, cmap='gray')\n",
    "plt.title('vertical gradient')\n",
    "plt.subplot(143)\n",
    "plt.imshow(grad_v, cmap='gray')\n",
    "plt.title('horizontal gradient')\n",
    "# plt.subplot(144)\n",
    "# plt.imshow(Hcc, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using the clusters to simulate the kernels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_size = [3,5,7]\n",
    "\n",
    "def apply_filtering(frames, core, bias, kernel_size):\n",
    "    img_stack = []\n",
    "    pred_img = []\n",
    "    kernel = kernel_size[::-1]\n",
    "    for index, K in enumerate(kernel):\n",
    "        if not len(img_stack):\n",
    "            frame_pad = tf.pad(frames, paddings=[[0,0], [0,0], [K//2,K//2], [K//2,K//2], [0,0]], mode='constant')\n",
    "            for i in range(K):\n",
    "                for j in range(K):\n",
    "                    img_stack.append(frame_pad[:, :, i:i+height, j:j+width,:])\n",
    "            img_stack = tf.stack(img_stack, axis=-1)                 # (bs, N, h, w，color, K*K) \n",
    "        else:\n",
    "            # k_diff = (kernel[index - 1]**2 - kernel[index]**2) // 2\n",
    "            k_diff = (kernel[index-1] - kernel[index]) // 2\n",
    "            k_chosen = []\n",
    "            for i in range(k_diff, kernel[index-1]-k_diff):\n",
    "                k_chosen += [i*kernel[index-1]+j for j in range(k_diff, kernel[index-1]-k_diff)]\n",
    "            # img_stack = img_stack[..., k_diff:-k_diff]\n",
    "            img_stack = tf.convert_to_tensor(img_stack.numpy()[..., k_chosen])\n",
    "        pred_img.append(tf.reduce_sum(tf.math.multiply(core[K], img_stack), axis=-1, keepdims=False))\n",
    "    pred_img = tf.stack(pred_img, axis=0)                           # (nb_kernels, bs, N, h, w, color)\n",
    "    pred_img_i = tf.reduce_mean(pred_img, axis=0, keepdims=False)   # (bs, N, h, w, color)\n",
    "\n",
    "    pred_img_i += bias\n",
    "\n",
    "    pred_img = tf.reduce_mean(pred_img_i, axis=1, keepdims=False)          # (bs, h, w, color)\n",
    "    return pred_img, pred_img_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 3\n",
    "#kmeans = KMeans(n_clusters=n_clusters, random_state=0)\n",
    "gmm = GMM(n_components=n_clusters, random_state=0)\n",
    "\n",
    "for step, (batch_test_X, batch_test_Y) in enumerate(test_dataset.take(1)):\n",
    "    '''obtain core by kpn model'''\n",
    "    pred_test_Y, _, core = model(batch_test_X, tf.expand_dims(batch_test_X[...,0], axis=-1))\n",
    "    batch_size, N, height, width, color = tf.expand_dims(batch_test_X[...,0], axis=-1).shape \n",
    "    core, bias = model.kernel_pred._convert_dict(core, batch_size, N, height, width, color)\n",
    "#     print(core[3].shape, core[5].shape, core[7].shape, bias.shape)\n",
    "    core3_all = tf.reshape(core[3], [-1, 9]).numpy()\n",
    "    print(core3_all.shape)\n",
    "    \n",
    "    '''cluster the kernels by gmm and obtain the labels (or clustered kernels)'''\n",
    "    #kmeans.fit(core3_all)\n",
    "    #core3_all_clustered = kmeans.cluster_centers_[kmeans.labels_]  # use kmeans to cluster the kernels\n",
    "    \n",
    "    #gmm.fit(core3_all)\n",
    "    core3_all_clustered = gmm.fit_predict(core3_all)  # use gmm to cluster the kernels\n",
    "    core3_all_clustered = gmm.means_[core3_all_clustered]\n",
    "    core3_all_clustered = core3_all_clustered.reshape(batch_size, N, height, width, color, -1)\n",
    "    core3_all_clustered = dict({3: core3_all_clustered}) # use dict\n",
    "    print(core3_all_clustered[3].shape)\n",
    "    \n",
    "    '''apply filters'''\n",
    "    pred_test_Y3_clustered, _ = apply_filtering(batch_test_X, core3_all_clustered, bias, kernel_size = [3])\n",
    "    print(pred_test_Y3_clustered.shape)\n",
    "    \n",
    "#     pred_test_Y5, _ = apply_filtering(batch_test_X, core[5], bias, kernel_size = [5])\n",
    "#     pred_test_Y7, _ = apply_filtering(batch_test_X, core[7], bias, kernel_size = [7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,80))\n",
    "for i in range(16):\n",
    "    plt.subplot(16, 6, 6*i+1)\n",
    "    plt.imshow(batch_test_X[i, ...,0].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('noisy image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+2)\n",
    "    plt.imshow(batch_test_Y[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('ground truth')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+3)\n",
    "    plt.imshow(pred_test_Y[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('recovered image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+4)\n",
    "    plt.imshow(pred_test_Y3_clustered[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('recovered image by clustered kernels')\n",
    "    plt.axis('off')\n",
    "\n",
    "#     plt.subplot(16, 6, 6*i+5)\n",
    "#     plt.imshow(tf.reduce_mean(core[5][i], axis=-1).numpy().squeeze(), cmap='gray')\n",
    "#     plt.title('filter 5x5 {:.3f}'.format(tf.reduce_mean(core[5][i]).numpy().squeeze()))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(16, 6, 6*i+6)\n",
    "#     plt.imshow(tf.reduce_mean(core[7][i], axis=-1).numpy().squeeze(), cmap='gray')\n",
    "#     plt.title('filter 7x7 {:.3f}'.format(tf.reduce_mean(core[7][i]).numpy().squeeze()))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "# plt.savefig('./eval/' + sub_dir + '/kpn3/recovered_images_by_30clustered_kernels.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "'''fetch te patches over each pixel'''\n",
    "batch_test_X_flatten = []\n",
    "K = 3\n",
    "frame_pad = tf.pad(batch_test_X, paddings=[[0,0], [0,0], [K//2,K//2], [K//2,K//2], [0,0]], mode='constant')\n",
    "for i in range(K):\n",
    "    for j in range(K):\n",
    "        batch_test_X_flatten.append(frame_pad[:, :, i:i+height, j:j+width,:])\n",
    "batch_test_X_flatten = tf.stack(batch_test_X_flatten, axis=-1).numpy().reshape(-1,9)\n",
    "\n",
    "'''build the dictionaries'''\n",
    "batch_test_X_dict = dict()\n",
    "core3_all_dict = dict()\n",
    "for i in range(n_clusters):\n",
    "#     batch_test_X_dict[i] = batch_test_X_flatten[np.where(kmeans.labels_==i)[0]]\n",
    "#     core3_all_dict[i] = core3_all[np.where(kmeans.labels_==i)[0]]\n",
    "    batch_test_X_dict[i] = batch_test_X_flatten[np.where(gmm.predict(core3_all)==i)[0]]\n",
    "    core3_all_dict[i] = core3_all[np.where(gmm.predict(core3_all)==i)[0]]\n",
    "\n",
    "'''drax the patches and their corresponding kernels'''\n",
    "plt.figure(figsize=(5*n_clusters,10))\n",
    "for i in range(n_clusters):\n",
    "    plt.subplot(2,n_clusters,i+1)\n",
    "    plt.imshow(batch_test_X_dict[i].mean(axis=0).reshape(3,3), cmap='gray')\n",
    "    plt.title('patch label {}'.format(i), fontsize=20)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(2,n_clusters,i+n_clusters+1)\n",
    "    plt.imshow(core3_all_dict[i].mean(axis=0).reshape(3,3), cmap='gray')\n",
    "    plt.title('kernel label {}'.format(i), fontsize=20)\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''save the patches and their corresponding kernels to a txt file using pickle'''\n",
    "import pickle\n",
    "\n",
    "'''sum up the kernels'''\n",
    "batch_test_X_dict_sum = core3_all_dict_sum = dict()\n",
    "for i in range(n_clusters):\n",
    "    batch_test_X_dict_sum[i] = batch_test_X_dict[i].mean(axis=0)\n",
    "    core3_all_dict_sum[i] = core3_all_dict[i].mean(axis=0)\n",
    "\n",
    "'''remove nan from dictionaries'''\n",
    "def remove_nan_from_dict(my_dict): \n",
    "    nb_removed = 0\n",
    "    new_dict = my_dict.copy()\n",
    "    for k,v in my_dict.items():\n",
    "        for e in v:\n",
    "            if math.isnan(e):\n",
    "                nb_removed += 1\n",
    "                new_dict.pop(k)\n",
    "                break\n",
    "    print(nb_removed, 'items removed from the dictionary')\n",
    "    return new_dict\n",
    "batch_test_X_dict_sum = remove_nan_from_dict(batch_test_X_dict_sum)\n",
    "core3_all_dict_sum = remove_nan_from_dict(core3_all_dict_sum)\n",
    "\n",
    "with open(\"kernels/patches50.txt\",\"wb\") as f: # 'b' means opening file in binary mode\n",
    "    pickle.dump(batch_test_X_dict_sum, f)\n",
    "\n",
    "with open(\"kernels/kernels50.txt\",\"wb\") as f:\n",
    "    pickle.dump(core3_all_dict_sum, f)\n",
    "        \n",
    "# with open('kernels/kernels.txt', 'rb') as f:\n",
    "#     a = pickle.loads(f.read())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use the patches information to decide which kernels to apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 10\n",
    "gmm = GMM(n_components=n_clusters, random_state=0)\n",
    "\n",
    "for step, (batch_test_X, batch_test_Y) in enumerate(test_dataset.take(1)):\n",
    "    '''obtain core by kpn model'''\n",
    "    pred_test_Y, _, core = model(batch_test_X, tf.expand_dims(batch_test_X[...,0], axis=-1))\n",
    "    \n",
    "    batch_size, N, height, width, color = tf.expand_dims(batch_test_X[...,0], axis=-1).shape \n",
    "    core, bias = model.kernel_pred._convert_dict(core, batch_size, N, height, width, color)\n",
    "#     print(core[3].shape, core[5].shape, core[7].shape, bias.shape)\n",
    "    core3_all = tf.reshape(core[3], [-1, 9]).numpy()\n",
    "    print(core3_all.shape)\n",
    "    \n",
    "    '''obtain patches over each pixel'''\n",
    "    batch_test_X_flatten = []\n",
    "    K = 3\n",
    "    frame_pad = tf.pad(batch_test_X, paddings=[[0,0], [0,0], [K//2,K//2], [K//2,K//2], [0,0]], mode='constant')\n",
    "    for i in range(K):\n",
    "        for j in range(K):\n",
    "            batch_test_X_flatten.append(frame_pad[:, :, i:i+height, j:j+width,:])\n",
    "    batch_test_X_flatten = tf.stack(batch_test_X_flatten, axis=-1)       \n",
    "    batch_test_X_flatten = batch_test_X_flatten.numpy().reshape(-1, 9)\n",
    "    print(batch_test_X_flatten.shape)\n",
    "    \n",
    "    '''cluster the patches by gmm and obtain the labels'''\n",
    "    labels = gmm.fit_predict(batch_test_X_flatten)  # use gmm to cluster the kernels\n",
    "    \n",
    "    '''use the labels to cluster the kernels'''\n",
    "    core3_all_clustered_dict = dict()\n",
    "    for i in range(n_clusters):\n",
    "        core3_all_clustered_dict[i] = core3_all[labels==i].mean(axis=0) \n",
    "    core3_all_clustered = np.array([core3_all_clustered_dict[labels[i]] for i in range(core3_all.shape[0])])\n",
    "    core3_all_clustered = core3_all_clustered.reshape(batch_size, N, height, width, color, -1)\n",
    "    core3_all_clustered = dict({3: core3_all_clustered}) # use dict\n",
    "    print(core3_all_clustered[3].shape)\n",
    "    \n",
    "    '''apply filters'''\n",
    "    pred_test_Y3_clustered, _ = apply_filtering(batch_test_X, core3_all_clustered, bias, kernel_size = [3])\n",
    "    print(pred_test_Y3_clustered.shape)\n",
    "    \n",
    "#     pred_test_Y5, _ = apply_filtering(batch_test_X, core[5], bias, kernel_size = [5])\n",
    "#     pred_test_Y7, _ = apply_filtering(batch_test_X, core[7], bias, kernel_size = [7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,80))\n",
    "for i in range(16):\n",
    "    plt.subplot(16, 6, 6*i+1)\n",
    "    plt.imshow(batch_test_X[i, ...,0].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('noisy image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+2)\n",
    "    plt.imshow(batch_test_Y[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('ground truth')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+3)\n",
    "    plt.imshow(pred_test_Y[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('recovered image')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    plt.subplot(16, 6, 6*i+4)\n",
    "    plt.imshow(pred_test_Y3_clustered[i].numpy().squeeze(), cmap='gray')\n",
    "    plt.title('recovered image by simulated kernels')\n",
    "    plt.axis('off')\n",
    "\n",
    "#     plt.subplot(16, 6, 6*i+5)\n",
    "#     plt.imshow(tf.reduce_mean(core[5][i], axis=-1).numpy().squeeze(), cmap='gray')\n",
    "#     plt.title('filter 5x5 {:.3f}'.format(tf.reduce_mean(core[5][i]).numpy().squeeze()))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(16, 6, 6*i+6)\n",
    "#     plt.imshow(tf.reduce_mean(core[7][i], axis=-1).numpy().squeeze(), cmap='gray')\n",
    "#     plt.title('filter 7x7 {:.3f}'.format(tf.reduce_mean(core[7][i]).numpy().squeeze()))\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#plt.savefig('./eval/' + sub_dir + '/kpn3/recovered_images_by_10clustered_kernels.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2gpu",
   "language": "python",
   "name": "tf2gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
